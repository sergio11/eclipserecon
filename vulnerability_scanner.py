import asyncio
import aiohttp
import os
from utils.logger import appLogger
from tqdm.asyncio import tqdm

class WebVulnerabilityScanner:
    """
    Class for scanning web vulnerabilities by fuzzing common sensitive files, backups, and other exposed resources.
    The scan is performed asynchronously to speed up the discovery process.

    Attributes:
        base_url (str): The base URL to fuzz with words from the wordlists.
        timeout (int): Maximum wait time for a response from the server.
        follow_redirects (bool): Whether to follow HTTP redirects.
        verify_ssl (bool): Whether to verify the SSL certificate.
        user_agent (str): The user-agent string to use for requests.
    """
    
    def __init__(self, base_url, timeout=5, follow_redirects=False, verify_ssl=False):
        """
        Initializes WebVulnerabilityScanner with the provided parameters.

        Args:
            base_url (str): Base URL for fuzzing content endpoints.
            timeout (int, optional): Timeout for requests. Defaults to 5 seconds.
            follow_redirects (bool, optional): Whether to follow redirects. Defaults to False.
            verify_ssl (bool, optional): Whether to verify SSL certificates. Defaults to False.
        """
        self.base_url = base_url
        self.timeout = timeout
        self.follow_redirects = follow_redirects
        self.verify_ssl = verify_ssl
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"
        self.wordlists = self._load_wordlists("assets/webcontent")  # Dynamically load wordlist files

    def start_scan(self, max_concurrent_requests=10):
        """Starts the scan process synchronously and returns the scan results.

        Args:
            max_concurrent_requests (int): The maximum number of simultaneous requests (concurrency).
        """
        appLogger.info(f"üî• Initiating breach detection on {self.base_url}... Activating fuzzing protocol.")
        result = asyncio.run(self._run_scan(max_concurrent_requests))
        appLogger.info(f"‚úÖ Scan complete. Found hidden entry points.")
        return result

    async def _run_scan(self, max_concurrent_requests):
        """Performs the web vulnerability scan asynchronously.

        Args:
            max_concurrent_requests (int): Maximum number of simultaneous requests.
        """
        appLogger.info("üíª Starting multi-threaded fuzzing process... Prepare for impact.")
        urls = self._generate_urls()
        semaphore = asyncio.Semaphore(max_concurrent_requests) 
        
        # Usa tqdm para la barra de progreso manual
        with tqdm(total=len(urls), desc="üöÄ Scanning targets", unit="url") as pbar:
            async with aiohttp.ClientSession(headers={'User-Agent': self.user_agent}, connector=aiohttp.TCPConnector(ssl=self.verify_ssl)) as session:
                tasks = []
                for url in urls:
                    task = asyncio.create_task(self._fetch_and_log(session, url, semaphore, pbar))
                    tasks.append(task)
                responses = await asyncio.gather(*tasks)
                appLogger.info("‚ö° Responses received. Analyzing... (Hold tight)")
                return self._process_responses(responses)

    async def _fetch_and_log(self, session, url, semaphore, pbar):
        """Makes an HTTP request to the specified URL and logs the response.

        Args:
            session (aiohttp.ClientSession): The HTTP session used for making requests.
            url (str): The URL to request.
            semaphore (asyncio.Semaphore): Semaphore to limit concurrent requests.
            pbar (tqdm): tqdm progress bar instance.

        Returns:
            tuple: URL, HTTP status code, response content or error message.
        """
        async with semaphore:
            try:
                response = await session.get(url, allow_redirects=self.follow_redirects, timeout=aiohttp.ClientTimeout(total=self.timeout))
                pbar.update(1)
                return (url, response.status, await response.text())
            except Exception as e:
                pbar.update(1)
                return (url, None, str(e))

    def _load_wordlists(self, dir_path):
        """Dynamically loads all fuzz wordlist files in the given directory.

        Args:
            dir_path (str): Path to the directory containing fuzz wordlist files.

        Returns:
            list: List of paths to wordlist files.
        """
        wordlists = []
        try:
            for file_name in os.listdir(dir_path):
                wordlists.append(os.path.join(dir_path, file_name))
            if not wordlists:
                appLogger.warning(f"‚ö†Ô∏è No fuzz wordlists found in {dir_path}. This could impact the scan.")
        except FileNotFoundError:
            appLogger.error(f"‚ùå Directory '{dir_path}' not found. Unable to load wordlists.")
            raise
        return wordlists

    def _generate_urls(self):
        """Generates URLs by fuzzing the base URL with words from multiple wordlists.

        Returns:
            list: List of generated URLs for fuzzing.
        """
        appLogger.info("üïµÔ∏è‚Äç‚ôÇÔ∏è Generating attack surface... Parsing wordlists for potential vulnerabilities.")
        urls = []
        total_wordlists = len(self.wordlists)
        
        for wordlist_path in self.wordlists:
            try:
                with open(wordlist_path, 'r') as file:
                    for line in file:
                        word = line.strip()
                        base = self.base_url.replace("FUZZ", word)
                        urls.append(base)
            except FileNotFoundError:
                appLogger.error(f"‚ùå Wordlist '{wordlist_path}' is missing. System breach compromised.")
        
        appLogger.info(f"üîé Processed {total_wordlists} wordlists. üí• {len(urls)} URLs crafted for fuzzing. Full assault initiated.")
        return urls

    def _process_responses(self, responses):
        """Processes the responses from the scan and returns the found URLs.

        Args:
            responses (list): List of responses from the scan (URL, status, content).

        Returns:
            list: A list of URLs that returned HTTP status 200.
        """
        appLogger.info("üîç Analyzing responses... Searching for open doors.")
        found_urls = []
        for url, status, content in responses:
            if status == 200:
                found_urls.append(url)
        return found_urls


if __name__ == "__main__":
    scanner = WebVulnerabilityScanner(
        base_url="http://192.168.11.130:8080/FUZZ",
        follow_redirects=True,
        verify_ssl=False
    )
    results = scanner.start_scan(max_concurrent_requests=10)
    # Print the results (list of found URLs)
    print("Found URLs:")
    for url in results:
        print(url)