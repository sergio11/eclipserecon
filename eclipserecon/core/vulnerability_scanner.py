from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import os
from eclipserecon.utils.logger import appLogger
from tqdm import tqdm

class WebVulnerabilityScanner:
    """
    Class for scanning web vulnerabilities by fuzzing common sensitive files, backups, and other exposed resources.
    The scan is performed concurrently using threads to speed up the discovery process.
    
    Attributes:
        base_url (str): The base URL to fuzz with words from the wordlists.
        scan_depth (str): The depth of the scan ('test', 'basic', 'normal', 'deep').
        timeout (int): Maximum wait time for a response from the server.
        follow_redirects (bool): Whether to follow HTTP redirects.
        verify_ssl (bool): Whether to verify the SSL certificate.
        user_agent (str): The user-agent string to use for requests.
    """
    
    def __init__(self, base_url, scan_depth="normal", timeout=5, follow_redirects=False, verify_ssl=False):
        """
        Initializes WebVulnerabilityScanner with the provided parameters.
        
        Args:
            base_url (str): Base URL for fuzzing content endpoints.
            timeout (int, optional): Timeout for requests. Defaults to 5 seconds.
            follow_redirects (bool, optional): Whether to follow redirects. Defaults to False.
            verify_ssl (bool, optional): Whether to verify SSL certificates. Defaults to False.
        """
        self.base_url = base_url
        self.timeout = timeout
        self.follow_redirects = follow_redirects
        self.verify_ssl = verify_ssl
        self.scan_depth = scan_depth
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"
        current_directory = os.path.dirname(os.path.abspath(__file__))
        self.wordlists = self._load_wordlists(os.path.join(current_directory, "assets/webcontent"))

    def start_scan(self, max_concurrent_requests=10):
        """Starts the scan process concurrently and returns the scan results.
        
        Args:
            max_concurrent_requests (int): The maximum number of simultaneous requests (concurrency).
        """
        appLogger.info(f"üî• Initiating breach detection on {self.base_url}... Activating fuzzing protocol.")
        result = self._run_scan(max_concurrent_requests)
        appLogger.info(f"‚úÖ Scan complete. Found hidden entry points.")
        return result

    def _run_scan(self, max_concurrent_requests):
        """Performs the web vulnerability scan concurrently using futures.
        
        Args:
            max_concurrent_requests (int): Maximum number of simultaneous requests.
        """
        appLogger.info("üíª Starting multi-threaded fuzzing process... Prepare for impact.")
        urls = self._generate_urls()

        with ThreadPoolExecutor(max_workers=max_concurrent_requests) as executor:
            with tqdm(total=len(urls), desc="üöÄ Scanning targets", unit="url") as pbar:
                future_to_url = {
                    executor.submit(self._fetch_and_log, url, pbar): url for url in urls
                }

                responses = []
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        response = future.result()
                        responses.append(response)
                    except Exception as e:
                        appLogger.error(f"‚ùå Error occurred with {url}: {e}")
                        responses.append((url, None, str(e)))
                    pbar.update(1)

                appLogger.info("‚ö° Responses received. Analyzing... (Hold tight)")
                return self._process_responses(responses)

    def _fetch_and_log(self, url, pbar):
        """Makes an HTTP request to the specified URL and logs the response.
        
        Args:
            url (str): The URL to request.
            pbar (tqdm): tqdm progress bar instance.
        
        Returns:
            tuple: URL, HTTP status code, response content or error message.
        """
        try:
            response = requests.get(url, allow_redirects=self.follow_redirects, timeout=self.timeout, verify=self.verify_ssl)
            return (url, response.status_code, response.text)
        except requests.RequestException as e:
            return (url, None, str(e))

    def _load_wordlists(self, dir_path):
        """Dynamically loads the appropriate wordlist files based on scan depth.
        
        Args:
            dir_path (str): Path to the directory containing fuzz wordlist files.
        
        Returns:
            list: List of paths to wordlist files to be used for fuzzing.
        """
        wordlists = []
        all_wordlists = [
            "Passwords.fuzz.txt", "api-endpoints.txt", "Apache.fuzz.txt", "Common-DB-Backups.txt",
            "logins.fuzz.txt", "php.fuzz.txt", "wordpress.fuzz.txt", "wp-plugins.fuzz.txt", 
            "nginx.txt", "Django.txt", "spring-boot.txt", "swagger.txt", "Drupal.txt", 
            "Common-PHP-Filenames.txt", "CommonBackdoors-PHP.fuzz.txt", "CommonBackdoors-ASP.fuzz.txt", 
            "CommonBackdoors-JSP.fuzz.txt", "UnixDotfiles.fuzz.txt", "Randomfiles.fuzz.txt", 
            "Frontpage.fuzz.txt"
        ]

        # Based on scan depth, load different sets of wordlists
        if self.scan_depth == "test":
            wordlists = [
                "Passwords.fuzz.txt", "api-endpoints.txt", "Apache.fuzz.txt", 
                "Common-DB-Backups.txt", "logins.fuzz.txt"
            ]
        elif self.scan_depth == "basic":
            wordlists = [
                "Passwords.fuzz.txt", "api-endpoints.txt", "Apache.fuzz.txt", 
                "Common-DB-Backups.txt", "logins.fuzz.txt", "php.fuzz.txt", 
                "wordpress.fuzz.txt"
            ]
        elif self.scan_depth == "normal":
            wordlists = [
                "Passwords.fuzz.txt", "api-endpoints.txt", "Apache.fuzz.txt", 
                "Common-DB-Backups.txt", "logins.fuzz.txt", "php.fuzz.txt", 
                "wordpress.fuzz.txt", "wp-plugins.fuzz.txt", "nginx.txt", "Django.txt"
            ]
        elif self.scan_depth == "deep":
            wordlists = all_wordlists
        else:
            appLogger.warning(f"‚ö†Ô∏è Unknown scan depth '{self.scan_depth}', defaulting to 'normal'.")
            wordlists = [
                "Passwords.fuzz.txt", "api-endpoints.txt", "Apache.fuzz.txt", 
                "Common-DB-Backups.txt", "logins.fuzz.txt", "php.fuzz.txt", 
                "wordpress.fuzz.txt", "wp-plugins.fuzz.txt", "nginx.txt", "Django.txt"
            ]

        wordlist_paths = [os.path.join(dir_path, file) for file in wordlists]
        
        appLogger.info(f"üîé Loaded {len(wordlist_paths)} wordlists for '{self.scan_depth}' depth scan.")
        return wordlist_paths

    def _generate_urls(self):
        """Generates URLs by fuzzing the base URL with words from selected wordlists.
        
        Returns:
            list: List of generated URLs for fuzzing.
        """
        appLogger.info("üïµÔ∏è‚Äç‚ôÇÔ∏è Generating attack surface... Parsing wordlists for potential vulnerabilities.")
        urls = []
        
        for wordlist_path in self.wordlists:
            try:
                with open(wordlist_path, 'r') as file:
                    for line in file:
                        word = line.strip()
                        base = self.base_url.replace("FUZZ", word)
                        urls.append(base)
            except FileNotFoundError:
                appLogger.error(f"‚ùå Wordlist '{wordlist_path}' is missing. System breach compromised.")
        
        appLogger.info(f"üîé Processed {len(self.wordlists)} wordlists. üí• {len(urls)} URLs crafted for fuzzing. Full assault initiated.")
        return urls

    def _process_responses(self, responses):
        """Processes the responses from the scan and returns the found URLs.
        
        Args:
            responses (list): List of responses from the scan (URL, status, content).
        
        Returns:
            list: A list of URLs that returned HTTP status 200.
        """
        appLogger.info("üîç Analyzing responses... Searching for open doors.")
        found_urls = []
        for url, status, content in responses:
            if status == 200:
                found_urls.append(url)
        return found_urls